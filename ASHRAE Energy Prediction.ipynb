{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASHRAE - Great Energy Predictor Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook builds a LightGBM model for predicting energy comsumption of buildings across various sites using past recorded hourly steam, chilled, electric meter readings. \n",
    "\n",
    "The notebook consits of the following section\n",
    "\n",
    "1. Reading the data\n",
    "2. Data Preprocessing\n",
    "3. Feature Engineering\n",
    "4. Validating the LightGBM  model\n",
    "5. Scoring the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from meteocalc import feels_like, Temp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "path_train = \"train.csv\"\n",
    "path_test = \"test.csv\"\n",
    "path_building = \"building_metadata.csv\"\n",
    "path_weather_train = \"weather_train.csv\"\n",
    "path_weather_test = \"weather_test.csv\"\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading train data\n",
    "Reading train data along with building and weather metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path_train)\n",
    "building = pd.read_csv(path_building)\n",
    "le = LabelEncoder()\n",
    "building.primary_use = le.fit_transform(building.primary_use)\n",
    "weather_train = pd.read_csv(path_weather_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20216100, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "## Memory optimization\n",
    "\n",
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "# Modified to support timestamp type, categorical type\n",
    "# Modified to add option to use float16\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 616.95 MB\n",
      "Memory usage after optimization is: 173.90 MB\n",
      "Decreased by 71.8%\n",
      "Memory usage of dataframe is 0.07 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 74.9%\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train, use_float16=True)\n",
    "building = reduce_mem_usage(building, use_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowstodrop = pd.read_csv('rows_to_drop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(rowstodrop.loc[:, '0'], inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19229841, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "There are two files with features that need to be merged with the data. One is building metadata that has information on the buildings and the other is weather data that has information on the weather. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new Features\n",
    "weather_train[\"datetime\"] = pd.to_datetime(weather_train[\"timestamp\"])\n",
    "weather_train[\"day\"] = weather_train[\"datetime\"].dt.day\n",
    "weather_train[\"week\"] = weather_train[\"datetime\"].dt.week\n",
    "weather_train[\"month\"] = weather_train[\"datetime\"].dt.month\n",
    "    \n",
    "# Reset Index for Fast Update\n",
    "weather_train = weather_train.set_index(['site_id','day','month'])\n",
    "\n",
    "air_temperature_filler = pd.DataFrame(weather_train.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "weather_train.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "# Step 1\n",
    "cloud_coverage_filler = weather_train.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "# Step 2\n",
    "cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "weather_train.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "due_temperature_filler = pd.DataFrame(weather_train.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "weather_train.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "# Step 1\n",
    "sea_level_filler = weather_train.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "# Step 2\n",
    "sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "weather_train.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "wind_direction_filler =  pd.DataFrame(weather_train.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "weather_train.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "wind_speed_filler =  pd.DataFrame(weather_train.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "weather_train.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "# Step 1\n",
    "precip_depth_filler = weather_train.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "# Step 2\n",
    "precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "weather_train.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "weather_train = weather_train.reset_index()\n",
    "weather_train = weather_train.drop(['datetime','day','week','month'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestampalign(df):\n",
    "    \"\"\"\n",
    "    Aligning timestamp for weather data based on UTC offsets of estimated site locations.\n",
    "    \"\"\"\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.loc[(df['site_id']==0) | (df['site_id']==3) | (df['site_id']==6) | (df['site_id']==7) | (df['site_id']==8) | (df['site_id']==11) | (df['site_id']==14) | (df['site_id']==15), 'timestamp'] = df['timestamp'] - pd.Timedelta(4, unit='h')\n",
    "    df.loc[(df['site_id']==2) | (df['site_id']==4) | (df['site_id']==10), 'timestamp'] = df['timestamp'] - pd.Timedelta(7, unit='h')\n",
    "    df.loc[(df['site_id']==9) | (df['site_id']==13), 'timestamp'] = df['timestamp'] - pd.Timedelta(5, unit='h')\n",
    "\n",
    "def prepare_data(X, building_data, weather_data, test=False):\n",
    "    \"\"\"\n",
    "    Preparing final dataset with all features.\n",
    "    \"\"\"\n",
    "    X.timestamp = pd.to_datetime(X.timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    #Filling NaNs using interpolation\n",
    "    #weather_data = weather_data.groupby(\"site_id\").apply(lambda group: group.interpolate(limit_direction=\"both\"))\n",
    "    \n",
    "    weather_data['relative_humidity'] = 100 * (np.exp((17.625 * weather_data['dew_temperature']) / (243.04 + weather_data['dew_temperature']))\n",
    "                                     / np.exp((17.625 * weather_data['air_temperature'])/(243.04 + weather_data['air_temperature'])))\n",
    "    \n",
    "    def calculate_fl(df):\n",
    "            flike_final = []\n",
    "            flike = []\n",
    "            # calculate Feels Like temperature\n",
    "            for i in range(len(df)):\n",
    "                at = df['air_temperature'][i]\n",
    "                rh = df['relative_humidity'][i]\n",
    "                ws = df['wind_speed'][i]\n",
    "                flike.append(feels_like(Temp(at, unit = 'C'), rh, ws))\n",
    "            for i in range(len(flike)):\n",
    "                flike_final.append(flike[i].f)\n",
    "            df['feels_like'] = flike_final\n",
    "            del flike_final, flike, at, rh, ws\n",
    "    calculate_fl(weather_data)\n",
    "    \n",
    "    X = X.merge(building_data, on=\"building_id\", how=\"left\")\n",
    "    X = X.merge(weather_data, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
    "    \n",
    "    X.sort_values(\"timestamp\")\n",
    "    X.reset_index(drop=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "#     holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "#                 \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "#                 \"2017-01-02\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "#                 \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "#                 \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "#                 \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "#                 \"2019-01-01\"]\n",
    "    \n",
    "    X.square_feet = np.log1p(X.square_feet)\n",
    "    \n",
    "    X[\"hour\"] = X.timestamp.dt.hour\n",
    "    X[\"weekday\"] = X.timestamp.dt.weekday\n",
    "    X[\"month\"] = X.timestamp.dt.month\n",
    "    X['month'].replace((1,2,3), 1, inplace=True)\n",
    "    X['month'].replace((4,5,6), 2, inplace=True)\n",
    "    X['month'].replace((7,8,9), 3, inplace=True)\n",
    "    X['month'].replace((10,11,12), 4, inplace=True)\n",
    "    #X[\"is_holiday\"] = (X.timestamp.isin(holidays)).astype(int)\n",
    "    #X.loc[(X['weekday'] == 5) | (X['weekday'] == 6), 'isWeekend'] = 1\n",
    "    #X['isWeekend'] = X['isWeekend'].fillna(0)\n",
    "    \n",
    "    drop_features = [\"timestamp\"]\n",
    "\n",
    "    if test:\n",
    "        row_ids = X.row_id\n",
    "        X.drop(\"row_id\", axis=1, inplace=True)\n",
    "        X.drop(drop_features, axis=1, inplace=True)\n",
    "        return X, row_ids\n",
    "    else:\n",
    "        X = X.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\n",
    "        X = X.query('not (building_id == 1099 & meter == 2 & meter_reading > 3e4)')\n",
    "        y = np.log1p(X.meter_reading)\n",
    "        X.drop(\"meter_reading\", axis=1, inplace=True)\n",
    "        X.drop(drop_features, axis=1, inplace=True)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestampalign(weather_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = prepare_data(df_train, building, weather_train)\n",
    "\n",
    "del df_train, weather_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>site_id</th>\n",
       "      <th>primary_use</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>year_built</th>\n",
       "      <th>floor_count</th>\n",
       "      <th>air_temperature</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>dew_temperature</th>\n",
       "      <th>precip_depth_1_hr</th>\n",
       "      <th>sea_level_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>relative_humidity</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.832181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.9</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>90.575533</td>\n",
       "      <td>36.933764</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.589514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.9</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>90.575533</td>\n",
       "      <td>36.933764</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.589514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.9</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>90.575533</td>\n",
       "      <td>36.933764</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.487946</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.9</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>90.575533</td>\n",
       "      <td>36.933764</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.309352</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.9</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>90.575533</td>\n",
       "      <td>36.933764</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  meter  site_id  primary_use  square_feet  year_built  \\\n",
       "1          105      0        1            0    10.832181         NaN   \n",
       "2          106      0        1            0     8.589514         NaN   \n",
       "3          106      3        1            0     8.589514         NaN   \n",
       "4          107      0        1            0    11.487946      2005.0   \n",
       "5          108      0        1            0    11.309352      1913.0   \n",
       "\n",
       "   floor_count  air_temperature  cloud_coverage  dew_temperature  \\\n",
       "1          5.0              3.8             0.0              2.4   \n",
       "2          4.0              3.8             0.0              2.4   \n",
       "3          4.0              3.8             0.0              2.4   \n",
       "4         10.0              3.8             0.0              2.4   \n",
       "5          5.0              3.8             0.0              2.4   \n",
       "\n",
       "   precip_depth_1_hr  sea_level_pressure  wind_direction  wind_speed  \\\n",
       "1                0.0              1020.9           240.0         3.1   \n",
       "2                0.0              1020.9           240.0         3.1   \n",
       "3                0.0              1020.9           240.0         3.1   \n",
       "4                0.0              1020.9           240.0         3.1   \n",
       "5                0.0              1020.9           240.0         3.1   \n",
       "\n",
       "   relative_humidity  feels_like  hour  weekday  month  \n",
       "1          90.575533   36.933764     0        4      1  \n",
       "2          90.575533   36.933764     0        4      1  \n",
       "3          90.575533   36.933764     0        4      1  \n",
       "4          90.575533   36.933764     0        4      1  \n",
       "5          90.575533   36.933764     0        4      1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "C:\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.961825\tvalid_1's rmse: 1.04926\n",
      "[50]\ttraining's rmse: 0.726986\tvalid_1's rmse: 0.859611\n",
      "[75]\ttraining's rmse: 0.664619\tvalid_1's rmse: 0.822081\n",
      "[100]\ttraining's rmse: 0.63348\tvalid_1's rmse: 0.812239\n",
      "[125]\ttraining's rmse: 0.611409\tvalid_1's rmse: 0.808279\n",
      "[150]\ttraining's rmse: 0.596548\tvalid_1's rmse: 0.805918\n",
      "[175]\ttraining's rmse: 0.585239\tvalid_1's rmse: 0.804875\n",
      "[200]\ttraining's rmse: 0.576228\tvalid_1's rmse: 0.804335\n",
      "[225]\ttraining's rmse: 0.569297\tvalid_1's rmse: 0.804328\n",
      "[250]\ttraining's rmse: 0.563411\tvalid_1's rmse: 0.804159\n",
      "[275]\ttraining's rmse: 0.557315\tvalid_1's rmse: 0.80394\n",
      "[300]\ttraining's rmse: 0.552406\tvalid_1's rmse: 0.804042\n",
      "[325]\ttraining's rmse: 0.547756\tvalid_1's rmse: 0.804058\n",
      "Early stopping, best iteration is:\n",
      "[277]\ttraining's rmse: 0.556716\tvalid_1's rmse: 0.803898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.9779\tvalid_1's rmse: 0.988205\n",
      "[50]\ttraining's rmse: 0.747706\tvalid_1's rmse: 0.776744\n",
      "[75]\ttraining's rmse: 0.684132\tvalid_1's rmse: 0.730255\n",
      "[100]\ttraining's rmse: 0.650051\tvalid_1's rmse: 0.717574\n",
      "[125]\ttraining's rmse: 0.626643\tvalid_1's rmse: 0.712329\n",
      "[150]\ttraining's rmse: 0.61018\tvalid_1's rmse: 0.709716\n",
      "[175]\ttraining's rmse: 0.59771\tvalid_1's rmse: 0.70826\n",
      "[200]\ttraining's rmse: 0.588754\tvalid_1's rmse: 0.707705\n",
      "[225]\ttraining's rmse: 0.58086\tvalid_1's rmse: 0.707239\n",
      "[250]\ttraining's rmse: 0.574301\tvalid_1's rmse: 0.706963\n",
      "[275]\ttraining's rmse: 0.568396\tvalid_1's rmse: 0.70683\n",
      "[300]\ttraining's rmse: 0.562994\tvalid_1's rmse: 0.706825\n",
      "[325]\ttraining's rmse: 0.55838\tvalid_1's rmse: 0.706894\n",
      "Early stopping, best iteration is:\n",
      "[295]\ttraining's rmse: 0.564113\tvalid_1's rmse: 0.706776\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.979476\tvalid_1's rmse: 0.969597\n",
      "[50]\ttraining's rmse: 0.74844\tvalid_1's rmse: 0.761798\n",
      "[75]\ttraining's rmse: 0.682959\tvalid_1's rmse: 0.719286\n",
      "[100]\ttraining's rmse: 0.648257\tvalid_1's rmse: 0.709404\n",
      "[125]\ttraining's rmse: 0.624967\tvalid_1's rmse: 0.705894\n",
      "[150]\ttraining's rmse: 0.609435\tvalid_1's rmse: 0.704027\n",
      "[175]\ttraining's rmse: 0.59681\tvalid_1's rmse: 0.703129\n",
      "[200]\ttraining's rmse: 0.587297\tvalid_1's rmse: 0.702903\n",
      "[225]\ttraining's rmse: 0.579495\tvalid_1's rmse: 0.703034\n",
      "Early stopping, best iteration is:\n",
      "[199]\ttraining's rmse: 0.587548\tvalid_1's rmse: 0.702883\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.979445\tvalid_1's rmse: 0.978731\n",
      "[50]\ttraining's rmse: 0.749057\tvalid_1's rmse: 0.768727\n",
      "[75]\ttraining's rmse: 0.684042\tvalid_1's rmse: 0.724532\n",
      "[100]\ttraining's rmse: 0.64899\tvalid_1's rmse: 0.713798\n",
      "[125]\ttraining's rmse: 0.624807\tvalid_1's rmse: 0.710506\n",
      "[150]\ttraining's rmse: 0.608983\tvalid_1's rmse: 0.70834\n",
      "[175]\ttraining's rmse: 0.597136\tvalid_1's rmse: 0.707472\n",
      "[200]\ttraining's rmse: 0.587631\tvalid_1's rmse: 0.707359\n",
      "[225]\ttraining's rmse: 0.579823\tvalid_1's rmse: 0.707125\n",
      "[250]\ttraining's rmse: 0.573988\tvalid_1's rmse: 0.70704\n",
      "[275]\ttraining's rmse: 0.568794\tvalid_1's rmse: 0.707291\n",
      "Early stopping, best iteration is:\n",
      "[249]\ttraining's rmse: 0.574164\tvalid_1's rmse: 0.707035\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[25]\ttraining's rmse: 0.982386\tvalid_1's rmse: 0.981898\n",
      "[50]\ttraining's rmse: 0.752236\tvalid_1's rmse: 0.761415\n",
      "[75]\ttraining's rmse: 0.686999\tvalid_1's rmse: 0.714359\n",
      "[100]\ttraining's rmse: 0.650626\tvalid_1's rmse: 0.701558\n",
      "[125]\ttraining's rmse: 0.626201\tvalid_1's rmse: 0.697204\n",
      "[150]\ttraining's rmse: 0.609688\tvalid_1's rmse: 0.69493\n",
      "[175]\ttraining's rmse: 0.597355\tvalid_1's rmse: 0.693647\n",
      "[200]\ttraining's rmse: 0.58794\tvalid_1's rmse: 0.693454\n",
      "[225]\ttraining's rmse: 0.579628\tvalid_1's rmse: 0.693379\n",
      "[250]\ttraining's rmse: 0.573547\tvalid_1's rmse: 0.693237\n",
      "[275]\ttraining's rmse: 0.568179\tvalid_1's rmse: 0.693376\n",
      "Early stopping, best iteration is:\n",
      "[249]\ttraining's rmse: 0.57386\tvalid_1's rmse: 0.693234\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"weekday\"]\n",
    "features = [c for c in X_train.columns if c not in ['month']]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 1580,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.85,\n",
    "    \"reg_lambda\": 1,\n",
    "    \"metric\": \"rmse\",\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=False, random_state = 0)\n",
    "models = []\n",
    "for train_idx, val_idx in kf.split(X_train, X_train['month']):\n",
    "    train_feats = X_train[features].loc[train_idx]\n",
    "    train_target = y_train.loc[train_idx]\n",
    "    val_feats = X_train[features].loc[val_idx]\n",
    "    val_target = y_train.loc[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(train_feats, label=train_target, categorical_feature=categorical_features, free_raw_data=False)\n",
    "    lgb_val = lgb.Dataset(val_feats, label=val_target, categorical_feature=categorical_features, free_raw_data=False)\n",
    "    \n",
    "    model = lgb.train(params, train_set=lgb_train, num_boost_round=3000, valid_sets=[lgb_train, lgb_val], verbose_eval=25,\n",
    "                     early_stopping_rounds = 50)\n",
    "    models.append(model)\n",
    "    del train_feats, train_target, val_feats, val_target, lgb_train, lgb_val\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data\n",
    "Preparing test data with same features as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(path_test)\n",
    "weather_test = pd.read_csv(path_weather_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Add new Features\n",
    "weather_test[\"datetime\"] = pd.to_datetime(weather_test[\"timestamp\"])\n",
    "weather_test[\"day\"] = weather_test[\"datetime\"].dt.day\n",
    "weather_test[\"week\"] = weather_test[\"datetime\"].dt.week\n",
    "weather_test[\"month\"] = weather_test[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "weather_test = weather_test.set_index(['site_id','day','month'])\n",
    "\n",
    "air_temperature_filler = pd.DataFrame(weather_test.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "weather_test.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "cloud_coverage_filler = weather_test.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "weather_test.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "due_temperature_filler = pd.DataFrame(weather_test.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "weather_test.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "sea_level_filler = weather_test.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    # Step 2\n",
    "sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "weather_test.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "wind_direction_filler =  pd.DataFrame(weather_test.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "weather_test.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "wind_speed_filler =  pd.DataFrame(weather_test.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "weather_test.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "precip_depth_filler = weather_test.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "weather_test.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "weather_test = weather_test.reset_index()\n",
    "weather_test = weather_test.drop(['datetime','day','week','month'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1272.51 MB\n",
      "Memory usage after optimization is: 358.65 MB\n",
      "Decreased by 71.8%\n"
     ]
    }
   ],
   "source": [
    "timestampalign(weather_test)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "X_test, row_ids = prepare_data(df_test, building, weather_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_test, building, weather_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring test data\n",
    "Averaging predictions from the five fold models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 120/120 [7:20:31<00:00, 220.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41697600\n"
     ]
    }
   ],
   "source": [
    "iterations = 120\n",
    "set_size = len(X_test)\n",
    "batch_size = set_size // iterations\n",
    "pred = []\n",
    "for i in tqdm(range(iterations)):\n",
    "    pos = i*batch_size\n",
    "    fold_preds = [np.expm1(model.predict(X_test[features].iloc[pos : pos+batch_size])) for model in models]\n",
    "    pred.extend(np.mean(fold_preds, axis=0))\n",
    "\n",
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Preparing final file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": np.clip(pred, 0, a_max=None)})\n",
    "submission.to_csv(\"submissionashraemodel14.csv.gz\", index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
